{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(prediction, annotation, class_labels=None):\n",
    "    # if not class_labels: Keep getting error here...\n",
    "    if class_labels is None:\n",
    "        class_labels = np.unique(annotation)\n",
    "    \n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int)\n",
    "        \n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 3.1: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        \n",
    "    \n",
    "    char_to_int = dict((c, i) for i, c in enumerate(class_labels))\n",
    "    \n",
    "    pred = []\n",
    "    anno = []\n",
    "\n",
    "    for i in range(len(prediction)):\n",
    "        pred.append([char_to_int[char] for char in prediction[i]][0])\n",
    "        anno.append([char_to_int[char] for char in annotation[i]][0])\n",
    "    \n",
    "    for a, p in zip(anno, pred):\n",
    "        confusion[a][p] += 1\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([\"B\", \"B\", \"C\", \"A\", \"B\", \"B\", \"C\", \"A\", \"A\", \"B\",\"B\"])\n",
    "annotation = np.array([\"A\", \"B\", \"B\", \"B\", \"C\", \"B\", \"C\", \"C\", \"A\", \"C\",\"C\"])\n",
    "class_labels = np.array([\"C\", \"A\", \"B\"])\n",
    "cm = confusion_matrix(prediction, annotation, np.array([\"C\", \"A\", \"B\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion):\n",
    "        \"\"\" Computes the accuracy given a confusion matrix.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        confusion : np.array\n",
    "            The confusion matrix (C by C, where C is the number of classes).\n",
    "            Rows are ground truth per class, columns are predictions\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy (between 0.0 to 1.0 inclusive)\n",
    "        \"\"\"\n",
    "        \n",
    "        # feel free to remove this\n",
    "        total = 0\n",
    "        corr_class = 0\n",
    "        for i in range(len(confusion)):\n",
    "            corr_class = corr_class + confusion[i][i]\n",
    "            for j in range(len(confusion)):\n",
    "                total = total + confusion[i][j]\n",
    "            \n",
    "        accuracy = round(corr_class/total, 3)\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 3.2: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion):\n",
    "        \"\"\" Computes the precision score per class given a confusion matrix.\n",
    "        \n",
    "        Also returns the macro-averaged precision across classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        confusion : np.array\n",
    "            The confusion matrix (C by C, where C is the number of classes).\n",
    "            Rows are ground truth per class, columns are predictions.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A C-dimensional numpy array, with the precision score for each\n",
    "            class in the same order as given in the confusion matrix.\n",
    "        float\n",
    "            The macro-averaged precision score across C classes.   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialise array to store precision for C classes\n",
    "        p = np.zeros((len(confusion), ))\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 3.3: COMPLETE THIS METHOD **\n",
    "        ####################################################################### \n",
    "        t_pos = 0\n",
    "        tf_pos = 0\n",
    "        for i in range(len(confusion)):\n",
    "            t_pos = confusion[i][i]\n",
    "            for j in range(len(confusion)):\n",
    "                tf_pos = tf_pos + confusion[j][i]\n",
    "            p[i] = t_pos/tf_pos\n",
    "            tf_pos = 0\n",
    "        \n",
    "        \n",
    "        # You will also need to change this\n",
    "        total = 0\n",
    "        for i in range(len(p)):\n",
    "            total = total + p[i]\n",
    "        macro_p = total/len(p)\n",
    "\n",
    "        return (p, macro_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = precision(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(confusion):\n",
    "        \"\"\" Computes the recall score per class given a confusion matrix.\n",
    "        \n",
    "        Also returns the macro-averaged recall across classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        confusion : np.array\n",
    "            The confusion matrix (C by C, where C is the number of classes).\n",
    "            Rows are ground truth per class, columns are predictions.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A C-dimensional numpy array, with the recall score for each\n",
    "            class in the same order as given in the confusion matrix.\n",
    "        \n",
    "        float\n",
    "            The macro-averaged recall score across C classes.   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialise array to store recall for C classes\n",
    "        r = np.zeros((len(confusion), ))\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** TASK 3.4: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        t_pos = 0\n",
    "        tp_fn = 0\n",
    "        for i in range(len(confusion)):\n",
    "            t_pos = confusion[i][i]\n",
    "            for j in range(len(confusion)):\n",
    "                tp_fn = tp_fn + confusion[i][j]\n",
    "            r[i] = t_pos/tp_fn\n",
    "            tp_fn = 0\n",
    "        \n",
    "        # You will also need to change this \n",
    "        total = 0\n",
    "        for i in range(len(r)):\n",
    "            total = total + r[i]\n",
    "        macro_r = total/len(r)\n",
    "        \n",
    "        return (r, macro_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = recall(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(confusion):\n",
    "        \"\"\" Computes the f1 score per class given a confusion matrix.\n",
    "        \n",
    "        Also returns the macro-averaged f1-score across classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        confusion : np.array\n",
    "            The confusion matrix (C by C, where C is the number of classes).\n",
    "            Rows are ground truth per class, columns are predictions.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A C-dimensional numpy array, with the f1 score for each\n",
    "            class in the same order as given in the confusion matrix.\n",
    "        \n",
    "        float\n",
    "            The macro-averaged f1 score across C classes.   \n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialise array to store recall for C classes\n",
    "        f = np.zeros((len(confusion), ))\n",
    "        \n",
    "        #######################################################################\n",
    "        #                 ** YOUR TASK: COMPLETE THIS METHOD **\n",
    "        #######################################################################\n",
    "        p, macro = precision(confusion)\n",
    "        r, macro = recall(confusion)\n",
    "        \n",
    "        for i in range(len(confusion)): \n",
    "            f[i] = (2 * (p[i] * r[i]) / (p[i] + r[i]))\n",
    "        \n",
    "        # You will also need to change this    \n",
    "        total = 0\n",
    "        for i in range(len(f)):\n",
    "            total = total + f[i]\n",
    "        macro_f = total/len(f)\n",
    "        \n",
    "        return (f, macro_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "# Does this need to be selected randomly?\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = round(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        count = 0;\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "            count += 1\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'B']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\"B\", \"B\", \"C\", \"A\", \"B\", \"B\", \"C\", \"A\", \"A\", \"B\",\"B\", \"B\", \"C\", \"A\", \"B\", \"B\", \"C\", \"A\", \"A\", \"B\"])\n",
    "cv_split =cross_validation_split(data, folds=10)\n",
    "cv_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k, dataset):\n",
    "    folds = cross_validation_split(dataset, k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        train = folds.copy()\n",
    "        test = folds[i]\n",
    "        del train[i]\n",
    "        # Enter code for fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
